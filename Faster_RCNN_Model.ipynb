{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4eb07eb-d037-4936-8325-cf730d3a7fd2",
   "metadata": {},
   "source": [
    "# Faster RCNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5f4474-247b-4592-811f-1adfb8cda04e",
   "metadata": {},
   "source": [
    "I will be using the Pascal Visual Object Classes 2007 Dataset\n",
    "\n",
    "https://www.kaggle.com/datasets/zaraks/pascal-voc-2007?select=VOCtrainval_06-Nov-2007\n",
    "\n",
    "This model will be based off the guidelines outlined in this instructional video: \n",
    "https://www.youtube.com/watch?v=Qq1yfWDdj5Y&list=WL&index=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f01373-3015-4c3a-aab1-db9c9cab006f",
   "metadata": {},
   "source": [
    "## implement simple Faster-RCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "111e6697-6320-42f8-ae9d-98ffa00dce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a617ac92-4b6f-4b8c-9adc-6af9fabba37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "#choose the accelerator if available. On Mac so using mps.\n",
    "device= torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b699e268-a2e5-4b43-b9af-44a39c30813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_regression_pred_to_anchors_or_proposals(\n",
    "    box_transform_pred, anchors_or_proposals):\n",
    "\n",
    "    \"\"\"\n",
    "    box_transform_pred: (num_anchors_or_proposals, num_classes, 4)\n",
    "    anchors_or_proposals: (num_anchors_or_proposals, 4)\n",
    "\n",
    "    return > pred_boxes: (num_anchors_or_proposals, num_classes, 4)\n",
    "    \"\"\"\n",
    "\n",
    "    box_transform_pred= box_transform_pred.reshape(\n",
    "    box_transform_pred.size(0), -1, 4\n",
    "    )\n",
    "\n",
    "    #get cx, cy, w, h, from x1, y1, x2, y2\n",
    "    w= anchors_or_proposals[:, 2] - anchors_or_proposals[:, 0]\n",
    "    h= anchors_or_proposals[:, 3] - anchors_or_proposals[:, 1]\n",
    "    center_x= anchors_or_proposals[:, 0] + 0.5*w\n",
    "    center_y= anchors_or_proposals[:, 1] + 0.5*h\n",
    "\n",
    "    dx= box_transform_pred[..., 0]\n",
    "    dy= box_transform_pred[..., 1]\n",
    "    dw= box_transform_pred[..., 2]\n",
    "    dh= box_transform_pred[..., 3]\n",
    "    #dh > (num_anchors_or_proposals, num_classes)\n",
    "\n",
    "    pred_center_x= dx*w[:, None] + center_x[:, None]\n",
    "    pred_center_y= dy*h[:, None] + center_y[:, None]\n",
    "    pred_w= torch.exp(dw)*w[:, None]\n",
    "    pred_h= torch.exp(dh)*h[:, None]\n",
    "    #pred_center_x > (num_anchors_or_proposals, num_classes)\n",
    "\n",
    "    pred_box_x1= pred_center_x - 0.5*pred_w\n",
    "    pred_box_y1= pred_center_y - 0.5*pred_h\n",
    "    pred_box_x2= pred_center_x + 0.5*pred_w\n",
    "    pred_box_y2= pred_center_y + 0.5*pred_h\n",
    "\n",
    "    pred_boxes= torch.stack((\n",
    "        pred_box_x1,\n",
    "        pred_box_y1,\n",
    "        pred_box_x2,\n",
    "        pred_box_y2\n",
    "    ), dim= 2)\n",
    "    #pred_boxes > (num_anchors_or_proposals, num_classes, 4)\n",
    "\n",
    "    return pred_boxes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739c6a55-977d-4d0d-a32d-020d0d28f26d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac47a220-540b-4f21-81c2-402e70c4ef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Region Proposal Network class\n",
    "class RegionProposalNetwork(nn.Module):\n",
    "    #512 is input created by feature map output of the backbone we are using\n",
    "    def __init__(self, in_channels= 512):\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        #specify scales and aspect ratio's for the anchor boxes\n",
    "        self.scales= [128, 256, 512]\n",
    "        self.aspect_ratios= [0.5, 1, 2]\n",
    "        self.num_anchors= len(self.scales) * len(self.aspect_ratios)\n",
    "\n",
    "        # 3 layers used\n",
    "        #Layer1: 3x3 convolutional layer\n",
    "        self.rpn_conv= nn.Conv2d(in_channels,\n",
    "                                in_channels,\n",
    "                                kernel_size= 3,\n",
    "                                stride= 1,\n",
    "                                padding=1)\n",
    "        #Layer2: 1x1 classification layer\n",
    "        self.cls_layer= nn.Conv2d(in_channels,\n",
    "                                  self.num_anchors, \n",
    "                                  kernel_size= 1,\n",
    "                                  stride= 1)\n",
    "        #Layer#3: 1x1 regression layer\n",
    "        self.bbox_reg_layer= nn.Conv2d(in_channels,\n",
    "                                        self.num_anchors *4,\n",
    "                                        kernel_size= 1,\n",
    "                                        stride= 1)\n",
    "        \n",
    "        \"\"\"\n",
    "        relevant shapes\n",
    "        image              [1x3x600x800]\n",
    "        feat               [1x512x37x50]\n",
    "        target['bboxes']   [1x6x4]\n",
    "        target['labels']   [1x6]\n",
    "        cls_scores         [1x9x37x50]\n",
    "        box_transform_pred [1x36x37x50]\n",
    "        stride_h/w         [16]\n",
    "        base_anchors       [9x4]\n",
    "        anchors            [16650x4]\n",
    "        proposals          [16650x4]\n",
    "        \"\"\"\n",
    "\n",
    "        def filter_proposals(self, proposals, cls_scores, image_shape):\n",
    "            #pre NMS filtering\n",
    "            cls_scores= cls_scores.reshape(-1)\n",
    "            cls_scores= torch.sigmoid(cls_scores)\n",
    "            _, top_n_idx= cls_scores.topk(10000)\n",
    "            cls_scores= cls_scores[top_n_idx]\n",
    "            proposals= proposals[top_n_idx]\n",
    "\n",
    "            #clamp boxes to image boundry\n",
    "        \n",
    "        def generate_anchors(self, image, feat):\n",
    "            grid_h, grid_w= feat.shape[-2:]\n",
    "            image_h, image_w= image.shape[-2:]\n",
    "            \n",
    "            stride_h= torch.tensor(image_h // grid_h,\n",
    "                                  dtype= torch.int32, #mps can only handle 32 instead of cuda 64\n",
    "                                  device= feat.device)\n",
    "            \n",
    "            stride_w= torch.tensor(image_w // grid_w,\n",
    "                                  dtype= torch.int32,\n",
    "                                  device= feat.device)\n",
    "\n",
    "            scales= torch.as_tensor(self.scales,\n",
    "                                    dtype= feat.dtype,\n",
    "                                    device= feat.device)\n",
    "\n",
    "            aspect_ratios= torch.as_tensor(self.aspect_ratios,\n",
    "                                    dtype= feat.dtype,\n",
    "                                    device= feat.device)\n",
    "\n",
    "            #ensure h/w= aspect_ratios and h*w= 1\n",
    "            h_ratios= torch.sqrt(aspect_ratios)\n",
    "            w_ratios= 1/h_ratios\n",
    "\n",
    "            ws= (w_ratios[:, None] * scales[None, :]).view(-1)\n",
    "            hs= (h_ratios[:, None] * scales[None, :]).view(-1)\n",
    "\n",
    "            base_anchors= torch.stack([-ws, -hs, ws, hs], dim=1) /2\n",
    "            base_anchors= base_anchors.round()\n",
    "\n",
    "            #convert all base anchors to grid of all anchors through\n",
    "            #shifts in x axis (0,1,...,W_feat-1) * stride_w\n",
    "            shifts_x= torch.arange(0, grid_w,\n",
    "                                   dtype= torch.int32,\n",
    "                                   device= feat.device) * stride_w\n",
    "\n",
    "            #shifts in y axis (0,1,...,H_feat-1) * stride_h\n",
    "            shifts_y= torch.arange(0, grid_h,\n",
    "                                   dtype= torch.int32,\n",
    "                                   device= feat.device) * stride_h\n",
    "\n",
    "            shifts_y, shifts_x= torch.meshgrid(shifts_y, shifts_x,\n",
    "                                               indexing= 'ij')\n",
    "\n",
    "            #(H_feat, W_feat)\n",
    "            shifts_x= shifts_x.reshape(-1)\n",
    "            shifts_y= shifts_y.reshape(-1)\n",
    "            shifts= torch.stack((shifts_x,\n",
    "                                 shifts_y,\n",
    "                                 shifts_x,\n",
    "                                 shifts_y), dim= 1)\n",
    "            #shifts > (H_feat * W_feat, 4)\n",
    "\n",
    "            #base_anchors > (num_anchors_per_location, 4)\n",
    "            #shifts > (H_feat * W_feat, 4)\n",
    "            anchors= (shifts.view(-1,1,4) + base_anchors.view(1,-1,4))\n",
    "            #(H_feat * W_feat, num_anchors_per_location, 4)\n",
    "\n",
    "            anchors= anchors.reshape(-1, 4)\n",
    "            #anchors > (H_feat 8 W_feat * num_anchors_per_location, 4)\n",
    "            return anchors\n",
    "            \n",
    "        #forward pass\n",
    "        def forward(self, image, feat, target): \n",
    "            #call RPN layers\n",
    "            rpn_feat= nn.Relu()(self.rpn_conv(feat))\n",
    "            cls_scores= self.cls_layer(rpn_feat)\n",
    "            box_transform_pred= self.bbox_reg_layer(rpn_feat)\n",
    "\n",
    "            #generate anchors\n",
    "            anchors= self.generate_anchors(image, feat)\n",
    "\n",
    "            # cls_scores > (Batch, Number of anchors per location, H_feat, W_feat)\n",
    "            number_of_anchors_per_location= cls_scores.size(1)\n",
    "            cls_scores= cls_scores.permute(0,2,3,1)\n",
    "            cls_scores= cls_scores.reshape(-1,1)\n",
    "            #cls_score > (Batch*H_feat*W_feat*Number_of_anchors_per_location, 1)\n",
    "\n",
    "            #box_transform_pred > (Batch, number_of_anchors_per_location*4, H_feat, W_feat)\n",
    "            box_transform_pred=  box_transform_pred.view(\n",
    "                box_transform_pred.size(0),\n",
    "                number_of_anchors_per_location,\n",
    "                4,\n",
    "                rpn_feat.shape(-2),\n",
    "                rpn_feat.shape(-1)\n",
    "            )\n",
    "             \n",
    "            #box_transform_pred > (B*H_feat*W_feat*num_of_anchors_per_location, 4)\n",
    "\n",
    "            #transform generated anchors according to box_transform_pred\n",
    "            proposals= apply_regression_pred_to_anchors_or_proposals(\n",
    "                box_transform_pred.detach().reshape(-1,1,4),\n",
    "                anchors\n",
    "            )\n",
    "\n",
    "            proposals= proposals.reshape(proposals.size(0),4)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e8ece-3b2c-41d8-814e-c9f528ea9bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
