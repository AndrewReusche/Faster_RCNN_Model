{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4eb07eb-d037-4936-8325-cf730d3a7fd2",
   "metadata": {},
   "source": [
    "# Faster RCNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5f4474-247b-4592-811f-1adfb8cda04e",
   "metadata": {},
   "source": [
    "I will be using the Pascal Visual Object Classes 2007 Dataset\n",
    "\n",
    "https://www.kaggle.com/datasets/zaraks/pascal-voc-2007?select=VOCtrainval_06-Nov-2007\n",
    "\n",
    "This model will be based off the guidelines outlined in this instructional video: \n",
    "https://www.youtube.com/watch?v=Qq1yfWDdj5Y&list=WL&index=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f01373-3015-4c3a-aab1-db9c9cab006f",
   "metadata": {},
   "source": [
    "## implement simple Faster-RCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111e6697-6320-42f8-ae9d-98ffa00dce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import torch\n",
    "import torch as nn\n",
    "import torchvision\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a617ac92-4b6f-4b8c-9adc-6af9fabba37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "#choose the accelerator if available. On Mac so using mps.\n",
    "device= torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac47a220-540b-4f21-81c2-402e70c4ef70",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (965009234.py, line 38)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef forward(self, image, feat, target):\u001b[39m\n                                            ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "#create Region Proposal Network class\n",
    "class RegionProposalNetwork(nn.Module):\n",
    "    #512 is input created by feature map output of the backbone we are using\n",
    "    def __init__(self, in_channels= 512):\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        #specify scales and aspect ratio's for the anchor boxes\n",
    "        self.scales= [128, 256, 512]\n",
    "        self.aspect_ratios= [0.5, 1, 2]\n",
    "        self.num_anchors= len(self.scales) * len(self.aspect_ratios)\n",
    "\n",
    "        # 3 layers used\n",
    "        #Layer1: 3x3 convolutional layer\n",
    "        self.rpn_conv= nn.Conv2d(in_channels,\n",
    "                                in_channels,\n",
    "                                kernel_size= 3,\n",
    "                                stride= 1,\n",
    "                                padding=1)\n",
    "        #Layer2: 1x1 classification layer\n",
    "        self.cls_layer= nn.Conv2d(in_channels,\n",
    "                                  self.num_anchors, \n",
    "                                  kernel_size= 1,\n",
    "                                  stride= 1)\n",
    "        #Layer#3: 1x1 regression layer\n",
    "        self.bbox_reg_layer= nn.Conv2d(in_channels,\n",
    "                                        self.num_anchors *4,\n",
    "                                        kernel_size= 1,\n",
    "                                        stride= 1)\n",
    "        \"\"\"\n",
    "        relevant shapes\n",
    "        image              [1x3x600x800]\n",
    "        feat               [1x512x37x50]\n",
    "        target['bboxes']   [1x6x4]\n",
    "        target['labels']   [1x6]\n",
    "        cls_scores         [1x9x37x50]\n",
    "        box_transform_pred [1x36x37x50]\n",
    "        stride_h/w         [16]\n",
    "        base_anchors       [9x4]\n",
    "        anchors            [16650x4]\n",
    "        \"\"\"\n",
    "\n",
    "        def generate_anchors(self, image, feat):\n",
    "            grid_h, grid_w= feat.shape[-2:]\n",
    "            image_h, image_w= image.shape[-2:]\n",
    "            \n",
    "            stride_h= torch.tensor(image_h // grid_h,\n",
    "                                  dtype= torch.int32, #mps can only handle 32 instead of cuda 64\n",
    "                                  device= feat.device)\n",
    "            \n",
    "            stride_w= torch.tensor(image_w // grid_w,\n",
    "                                  dtype= torch.int32,\n",
    "                                  device= feat.device)\n",
    "\n",
    "            scales= torch.as_tensor(self.scales,\n",
    "                                    dtype= feat.dtype,\n",
    "                                    device= feat.device)\n",
    "\n",
    "            aspect_ratios= torch.as_tensor(self.aspect_ratios,\n",
    "                                    dtype= feat.dtype,\n",
    "                                    device= feat.device)\n",
    "\n",
    "            #ensure h/w= aspect_ratios and h*w= 1\n",
    "            h_ratios= torch.sqrt(aspect_ratios)\n",
    "            w_ratios= 1/h_ratios\n",
    "\n",
    "            ws= (w_ratios[:, None] * scales[None, :]).view(-1)\n",
    "            hs= (h_ratios[:, None] * scales[None, :]).view(-1)\n",
    "\n",
    "            base_anchors= torch.stack([-ws, -hs, ws, hs], dim=1) /2\n",
    "            base_anchors= base_anchors.round()\n",
    "\n",
    "            #convert all base anchors to grid of all anchors through\n",
    "            #shifts in x axis (0,1,...,W_feat-1) * stride_w\n",
    "            shifts_x= torch.arange(0, grid_w,\n",
    "                                   dtype= torch.int32,\n",
    "                                   device= feat.device) * stride_w\n",
    "\n",
    "            #shifts in y axis (0,1,...,H_feat-1) * stride_h\n",
    "            shifts_y= torch.arange(0, grid_h,\n",
    "                                   dtype= torch.int32,\n",
    "                                   device= feat.device) * stride_h\n",
    "\n",
    "            shifts_y, shifts_x= torch.meshgrid(shifts_y, shifts_x,\n",
    "                                               indexing= 'ij')\n",
    "\n",
    "            #(H_feat, W_feat)\n",
    "            shifts_x= shifts_x.reshape(-1)\n",
    "            shifts_y= shifts_y.reshape(-1)\n",
    "            shifts= torch.stack((shifts_x,\n",
    "                                 shifts_y,\n",
    "                                 shifts_x,\n",
    "                                 shifts_y), dim= 1)\n",
    "            #shifts > (H_feat * W_feat, 4)\n",
    "\n",
    "            #base_anchors > (num_anchors_per_location, 4)\n",
    "            #shifts > (H_feat * W_feat, 4)\n",
    "            anchors= (shifts.view(-1,1,4) + base_anchors.view(1,-1,4))\n",
    "            #(H_feat * W_feat, num_anchors_per_location, 4)\n",
    "\n",
    "            anchors= anchors.reshape(-1, 4)\n",
    "            #anchors > (H_feat 8 W_feat * num_anchors_per_location, 4)\n",
    "            return anchors\n",
    "            \n",
    "        #forward pass\n",
    "        def forward(self, image, feat, target): \n",
    "            #call RPN layers\n",
    "            rpn_feat= nn.Relu()(self.rpn_conv(feat))\n",
    "            cls_scores= self.cls_layer(rpn_feat)\n",
    "            box_transform_pred= self.bbox_reg_layer(rpn_feat)\n",
    "\n",
    "            #generate anchors\n",
    "            anchors= self.generate_anchors(image, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e8ece-3b2c-41d8-814e-c9f528ea9bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
