{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4eb07eb-d037-4936-8325-cf730d3a7fd2",
   "metadata": {},
   "source": [
    "# Faster RCNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5f4474-247b-4592-811f-1adfb8cda04e",
   "metadata": {},
   "source": [
    "I will be using the Pascal Visual Object Classes 2007 Dataset\n",
    "\n",
    "https://www.kaggle.com/datasets/zaraks/pascal-voc-2007?select=VOCtrainval_06-Nov-2007\n",
    "\n",
    "This model will be based off the guidelines outlined in this instructional video: \n",
    "https://www.youtube.com/watch?v=Qq1yfWDdj5Y&list=WL&index=1 \n",
    "\n",
    "timestamp 42:41"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f01373-3015-4c3a-aab1-db9c9cab006f",
   "metadata": {},
   "source": [
    "## implement simple Faster-RCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "111e6697-6320-42f8-ae9d-98ffa00dce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a617ac92-4b6f-4b8c-9adc-6af9fabba37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "#choose the accelerator if available. On Mac so using mps.\n",
    "device= torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55715fbf-20bd-4657-bec3-f40fbe48946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(boxes1, boxes2):\n",
    "    '''\n",
    "    boxes1: (N x 4)\n",
    "    boxes2: (M x 4)\n",
    "\n",
    "    return: IOU matrix of shape (N x M)\n",
    "    '''\n",
    "\n",
    "    #area of boxes (x2-x1) * (y2-y1)\n",
    "    area1= (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1]) \n",
    "    area2= (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1]) \n",
    "\n",
    "    #get top left x1, y1\n",
    "    x_left= torch.max(boxes1[:, None, 0], boxes2[:, 0]) #(N, M)\n",
    "    y_top= torch.max(boxes1[:, None, 1], boxes2[:, 1]) #(N, M)\n",
    "\n",
    "    #get bottom right x2, y2\n",
    "    x_right= torch.min(boxes1[:, None, 2], boxes2[:, 2]) #(N, M)\n",
    "    y_bottom= torch.min(boxes1[:, None, 3], boxes2[:, 3]) #(N, M)\n",
    "\n",
    "    intersection_area= (x_right - x_left).clamp(min=0) * (y_bottom - y_top).clamp(min=0)\n",
    "    union = area1[:, None] + area2 - intersection_area\n",
    "    return intersection_area/union #(N, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b699e268-a2e5-4b43-b9af-44a39c30813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_regression_pred_to_anchors_or_proposals(\n",
    "    box_transform_pred, anchors_or_proposals):\n",
    "\n",
    "    \"\"\"\n",
    "    box_transform_pred: (num_anchors_or_proposals, num_classes, 4)\n",
    "    anchors_or_proposals: (num_anchors_or_proposals, 4)\n",
    "\n",
    "    return > pred_boxes: (num_anchors_or_proposals, num_classes, 4)\n",
    "    \"\"\"\n",
    "\n",
    "    box_transform_pred= box_transform_pred.reshape(\n",
    "    box_transform_pred.size(0), -1, 4\n",
    "    )\n",
    "\n",
    "    #get cx, cy, w, h, from x1, y1, x2, y2\n",
    "    w= anchors_or_proposals[:, 2] - anchors_or_proposals[:, 0]\n",
    "    h= anchors_or_proposals[:, 3] - anchors_or_proposals[:, 1]\n",
    "    center_x= anchors_or_proposals[:, 0] + 0.5*w\n",
    "    center_y= anchors_or_proposals[:, 1] + 0.5*h\n",
    "\n",
    "    dx= box_transform_pred[..., 0]\n",
    "    dy= box_transform_pred[..., 1]\n",
    "    dw= box_transform_pred[..., 2]\n",
    "    dh= box_transform_pred[..., 3]\n",
    "    #dh > (num_anchors_or_proposals, num_classes)\n",
    "\n",
    "    pred_center_x= dx*w[:, None] + center_x[:, None]\n",
    "    pred_center_y= dy*h[:, None] + center_y[:, None]\n",
    "    pred_w= torch.exp(dw)*w[:, None]\n",
    "    pred_h= torch.exp(dh)*h[:, None]\n",
    "    #pred_center_x > (num_anchors_or_proposals, num_classes)\n",
    "\n",
    "    pred_box_x1= pred_center_x - 0.5*pred_w\n",
    "    pred_box_y1= pred_center_y - 0.5*pred_h\n",
    "    pred_box_x2= pred_center_x + 0.5*pred_w\n",
    "    pred_box_y2= pred_center_y + 0.5*pred_h\n",
    "\n",
    "    pred_boxes= torch.stack((\n",
    "        pred_box_x1,\n",
    "        pred_box_y1,\n",
    "        pred_box_x2,\n",
    "        pred_box_y2\n",
    "    ), dim= 2)\n",
    "    #pred_boxes > (num_anchors_or_proposals, num_classes, 4)\n",
    "\n",
    "    return pred_boxes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "739c6a55-977d-4d0d-a32d-020d0d28f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp_boxes_to_image_boundary(boxes, image_shape):\n",
    "    boxes_x1= boxes[..., 0]\n",
    "    boxes_y1= boxes[..., 1]\n",
    "    boxes_x2= boxes[..., 2]\n",
    "    boxes_y2= boxes[..., 3]\n",
    "    \n",
    "    height, width= image_shape[-2:]\n",
    "    \n",
    "    boxes_x1= boxes_x1.clamp(min= 0, max= width)\n",
    "    boxes_x2= boxes_x2.clamp(min=0, max= width) \n",
    "    boxes_y1= boxes_y1.clamp(min= 0, max= height)\n",
    "    boxes_y2= boxes_y2.clamp(min= 0, max= height)\n",
    "\n",
    "    boxes= torch.cat((\n",
    "        boxes_x1[..., None],\n",
    "        boxes_x2[..., None],\n",
    "        boxes_y1[..., None],\n",
    "        boxes_y2[..., None]\n",
    "    ), dim= -1)\n",
    "    return boxes\n",
    "\n",
    "def boxes_to_transformation_targets(ground_truth_boxes, anchors_or_proposals):\n",
    "    #get center_x, center_y, w, h from x1, y1, x2, y2 for anchors\n",
    "    widths= anchors_or_proposals[:, 2] - anchors_or_proposals[:, 0]\n",
    "    heights= anchors_or_proposals[:, 3] - anchors_or_proposals[:, 1]\n",
    "    center_x= anchors_or_proposals[:, 0] + 0.5*widths\n",
    "    center_y= anchors_or_proposals[:, 1] + 0.5*heights\n",
    "\n",
    "    #get center_x, center_y, w, h from x1, y1, x2, y2 for gt boxes\n",
    "    gt_widths= ground_truth_boxes[:, 2] - ground_truth_boxes[:, 0]\n",
    "    gt_heights= ground_truth_boxes[:, 3] - ground_truth_boxes[:, 1]\n",
    "    gt_center_x= ground_truth_boxes[:, 0] + 0.5*gt_widths\n",
    "    gt_center_y= ground_truth_boxes[:, 1] + 0.5*gt_heights\n",
    "\n",
    "    target_dx= (gt_center_x - center_x) / widths\n",
    "    target_dy= (gt_center_y - center_y) / heights\n",
    "    target_dw= torch.log(gt_widths / widths)\n",
    "    target_dh= torch.log(gt_heights / heights)\n",
    "\n",
    "    regression_targets= torch.stack((\n",
    "        target_dx,\n",
    "        target_dy,\n",
    "        target_dw,\n",
    "        target_dh\n",
    "    ), dim= 1)\n",
    "\n",
    "    return regression_targets\n",
    "\n",
    "def sample_positive_negative(labels, positive_count, total_count):\n",
    "    positive= torch.where(labels >= 1)[0]\n",
    "    negative= torch.where(labels == 0)[0]\n",
    "    num_pos= positive_count\n",
    "    num_pos= min(positive.numel(), num_pos)\n",
    "    num_neg= total_count - num_pos\n",
    "    num_neg= min(negative.numel(), num_neg)\n",
    "\n",
    "    perm_positive_idxs= torch.randperm(positive.numel(),\n",
    "                                        device= positive.device)[:num_pos]\n",
    "    perm_negative_idxs= torch.randperm(negative.numel(),\n",
    "                                        device= negative.device)[:num_neg]\n",
    "    pos_idxs= positive[perm_positive_idxs]\n",
    "    neg_idxs= negative[perm_negative_idxs]\n",
    "    sampled_pos_idx_mask= torch.zeros_like(labels, dtype= torch.bool)\n",
    "    sampled_neg_idx_mask= torch.zeros_like(labels, dtype= torch.bool)\n",
    "    sampled_pos_idx_mask[pos_idxs]= True\n",
    "    sampled_neg_idx_mask[neg_idxs]= True\n",
    "\n",
    "    return sampled_neg_idx_mask, sampled_pos_idx_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac47a220-540b-4f21-81c2-402e70c4ef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Region Proposal Network class\n",
    "class RegionProposalNetwork(nn.Module):\n",
    "\n",
    "\"\"\"\n",
    "RPN Summary\n",
    "\n",
    "For Training and Inference:\n",
    "    Call RPN Layers\n",
    "    Generate Anchors\n",
    "    Convert Anchors to Proposals using Box Transformation Prediction\n",
    "    Filter Proposals\n",
    "\n",
    "For Training Only:\n",
    "    Assign Ground Truth Boxes to Anchors\n",
    "    Compute Labels and Regression Targets for Anchors\n",
    "    Sample Positive and Negative Anchors\n",
    "    Compute Classification Loss Using Sampled Anchors\n",
    "    Compute Localization Loss Using Sampled Positive Anchors\n",
    "\"\"\"\n",
    "    \n",
    "    #512 is input created by feature map output of the backbone we are using\n",
    "    def __init__(self, in_channels= 512):\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        #specify scales and aspect ratio's for the anchor boxes\n",
    "        self.scales= [128, 256, 512]\n",
    "        self.aspect_ratios= [0.5, 1, 2]\n",
    "        self.num_anchors= len(self.scales) * len(self.aspect_ratios)\n",
    "\n",
    "        # 3 layers used\n",
    "        #Layer1: 3x3 convolutional layer\n",
    "        self.rpn_conv= nn.Conv2d(in_channels,\n",
    "                                in_channels,\n",
    "                                kernel_size= 3,\n",
    "                                stride= 1,\n",
    "                                padding=1)\n",
    "        #Layer2: 1x1 classification layer\n",
    "        self.cls_layer= nn.Conv2d(in_channels,\n",
    "                                  self.num_anchors, \n",
    "                                  kernel_size= 1,\n",
    "                                  stride= 1)\n",
    "        #Layer#3: 1x1 regression layer\n",
    "        self.bbox_reg_layer= nn.Conv2d(in_channels,\n",
    "                                        self.num_anchors *4,\n",
    "                                        kernel_size= 1,\n",
    "                                        stride= 1)\n",
    "        \n",
    "        \"\"\"\n",
    "        relevant shapes\n",
    "        image              [1x3x600x800]\n",
    "        feat               [1x512x37x50]\n",
    "        target['bboxes']   [1x6x4]\n",
    "        target['labels']   [1x6]\n",
    "        cls_scores         [1x9x37x50]\n",
    "        box_transform_pred [1x36x37x50]\n",
    "        stride_h/w         [16]\n",
    "        base_anchors       [9x4]\n",
    "        anchors            [16650x4]\n",
    "        proposals          [16650x4]\n",
    "        scores             [2000,]\n",
    "        gt_boxes           [6x4]\n",
    "        iou_matrix         [6x16650]\n",
    "        matched_gt_boxes   [16650x4]\n",
    "        labels             [16650,]\n",
    "        regression_targets [16650x4]\n",
    "        \"\"\"\n",
    "\n",
    "        def assign_targets_to_anchors(self, anchors, gt_boxes):\n",
    "            #get (gt_boxes, num_anchors) IOU matrix\n",
    "            iou_matrix= get_iou(gt_boxes, anchors)\n",
    "\n",
    "            #for each anchor box get best gt box index\n",
    "            best_match_iou, best_match_gt_index= iou_matrix.max(dim=0)\n",
    "\n",
    "            #this copy will be needed later ot add low quality boxes\n",
    "            best_match_gt_idx_pre_threshold= best_match_gt_index.clone()\n",
    "\n",
    "            below_low_threshold= best_match_iou < 0.3\n",
    "            between_threshold= (best_match_iou >= 0.3) and (best_match_iou < 0.7)\n",
    "            best_match_gt_index[below_low_threshold]= -1\n",
    "            best_match_gt_index[between_threshold]= -2\n",
    "              \n",
    "            #low quality anchor boxes\n",
    "            best_anchor_iou_for_gt, _ = iou_matrix.max(dim=1)\n",
    "            gt_pred_pair_with_highest_iou= torch.where(iou_matrix == best_anchor_iou_for_gt[:, None])\n",
    "\n",
    "            #get all the anchors indexes to update\n",
    "            pred_inds_to_update= gt_pred_pair_with_highest_iou[1]\n",
    "            best_match_gt_index[pred_inds_to_update]= best_match_gt_idx_pre_threshold[pred_inds_to_update]\n",
    "            \n",
    "            #best match index is either valid or -1(background) or -2(to ignore)\n",
    "            matched_gt_boxes= gt_boxes[best_match_gt_index.clamp(min= 0)]\n",
    "\n",
    "            #set all foreground lanchor labels as 1\n",
    "            labels= best_match_gt_index >= 0\n",
    "            labels= labels.to(dtype= torch.float32)\n",
    "\n",
    "            #set all background labels as 0\n",
    "            background_anchors= best_match_gt_index == -1\n",
    "            labels[background_anchors]= 0.0\n",
    "\n",
    "            #set all to be ignored labels as -1\n",
    "            ingored_anchors= best_match_gt_index == -2\n",
    "            labels[ignored_anchors] = -1.0\n",
    "\n",
    "            #later for classification we pick labels which have >= 0\n",
    "            return labels, matched_gt_boxes\n",
    "            \n",
    "        \n",
    "        def filter_proposals(self, proposals, cls_scores, image_shape):\n",
    "            #pre NMS filtering\n",
    "            cls_scores= cls_scores.reshape(-1)\n",
    "            cls_scores= torch.sigmoid(cls_scores)\n",
    "            _, top_n_idx= cls_scores.topk(10000)\n",
    "            cls_scores= cls_scores[top_n_idx]\n",
    "            proposals= proposals[top_n_idx]\n",
    "\n",
    "            #clamp boxes to image boundary\n",
    "            proposals= clamp_boxes_to_image_boundary(proposals, image_shape)\n",
    "\n",
    "            #NMS based on objectness\n",
    "            keep_mask- torch.zeros_like(cls_scores, dtype= torch.bool)\n",
    "            keep_indices= torch.ops.torchvision.nms(proposals, \n",
    "                                                    cls_scores, \n",
    "                                                    0.7)\n",
    "\n",
    "            post_nms_keep_indices= keep_indices[\n",
    "                cls_scores[keep_indices].sort(descending= True)[1]\n",
    "                ]\n",
    "\n",
    "            #post NMS top k filtering\n",
    "            proposals= proposals[post_nms_keep_indices[:2000]]\n",
    "            cls_scores= cls_scores[post_nms_keep_indices[:2000]]\n",
    "            return proposals, cls_scores\n",
    "        \n",
    "        def generate_anchors(self, image, feat):\n",
    "            grid_h, grid_w= feat.shape[-2:]\n",
    "            image_h, image_w= image.shape[-2:]\n",
    "            \n",
    "            stride_h= torch.tensor(image_h // grid_h,\n",
    "                                  dtype= torch.int32, #mps can only handle 32 instead of cuda 64\n",
    "                                  device= feat.device)\n",
    "            \n",
    "            stride_w= torch.tensor(image_w // grid_w,\n",
    "                                  dtype= torch.int32,\n",
    "                                  device= feat.device)\n",
    "\n",
    "            scales= torch.as_tensor(self.scales,\n",
    "                                    dtype= feat.dtype,\n",
    "                                    device= feat.device)\n",
    "\n",
    "            aspect_ratios= torch.as_tensor(self.aspect_ratios,\n",
    "                                    dtype= feat.dtype,\n",
    "                                    device= feat.device)\n",
    "\n",
    "            #ensure h/w= aspect_ratios and h*w= 1\n",
    "            h_ratios= torch.sqrt(aspect_ratios)\n",
    "            w_ratios= 1/h_ratios\n",
    "\n",
    "            ws= (w_ratios[:, None] * scales[None, :]).view(-1)\n",
    "            hs= (h_ratios[:, None] * scales[None, :]).view(-1)\n",
    "\n",
    "            base_anchors= torch.stack([-ws, -hs, ws, hs], dim=1) /2\n",
    "            base_anchors= base_anchors.round()\n",
    "\n",
    "            #convert all base anchors to grid of all anchors through\n",
    "            #shifts in x axis (0,1,...,W_feat-1) * stride_w\n",
    "            shifts_x= torch.arange(0, grid_w,\n",
    "                                   dtype= torch.int32,\n",
    "                                   device= feat.device) * stride_w\n",
    "\n",
    "            #shifts in y axis (0,1,...,H_feat-1) * stride_h\n",
    "            shifts_y= torch.arange(0, grid_h,\n",
    "                                   dtype= torch.int32,\n",
    "                                   device= feat.device) * stride_h\n",
    "\n",
    "            shifts_y, shifts_x= torch.meshgrid(shifts_y, shifts_x,\n",
    "                                               indexing= 'ij')\n",
    "\n",
    "            #(H_feat, W_feat)\n",
    "            shifts_x= shifts_x.reshape(-1)\n",
    "            shifts_y= shifts_y.reshape(-1)\n",
    "            shifts= torch.stack((shifts_x,\n",
    "                                 shifts_y,\n",
    "                                 shifts_x,\n",
    "                                 shifts_y), dim= 1)\n",
    "            #shifts > (H_feat * W_feat, 4)\n",
    "\n",
    "            #base_anchors > (num_anchors_per_location, 4)\n",
    "            #shifts > (H_feat * W_feat, 4)\n",
    "            anchors= (shifts.view(-1,1,4) + base_anchors.view(1,-1,4))\n",
    "            #(H_feat * W_feat, num_anchors_per_location, 4)\n",
    "\n",
    "            anchors= anchors.reshape(-1, 4)\n",
    "            #anchors > (H_feat 8 W_feat * num_anchors_per_location, 4)\n",
    "            return anchors\n",
    "            \n",
    "        #forward pass\n",
    "        def forward(self, image, feat, target): \n",
    "            #call RPN layers\n",
    "            rpn_feat= nn.Relu()(self.rpn_conv(feat))\n",
    "            cls_scores= self.cls_layer(rpn_feat)\n",
    "            box_transform_pred= self.bbox_reg_layer(rpn_feat)\n",
    "\n",
    "            #generate anchors\n",
    "            anchors= self.generate_anchors(image, feat)\n",
    "\n",
    "            # cls_scores > (Batch, Number of anchors per location, H_feat, W_feat)\n",
    "            number_of_anchors_per_location= cls_scores.size(1)\n",
    "            cls_scores= cls_scores.permute(0,2,3,1)\n",
    "            cls_scores= cls_scores.reshape(-1,1)\n",
    "            #cls_score > (Batch*H_feat*W_feat*Number_of_anchors_per_location, 1)\n",
    "\n",
    "            #box_transform_pred > (Batch, number_of_anchors_per_location*4, H_feat, W_feat)\n",
    "            box_transform_pred=  box_transform_pred.view(\n",
    "                box_transform_pred.size(0),\n",
    "                number_of_anchors_per_location,\n",
    "                4,\n",
    "                rpn_feat.shape(-2),\n",
    "                rpn_feat.shape(-1)\n",
    "            )\n",
    "             \n",
    "            #box_transform_pred > (B*H_feat*W_feat*num_of_anchors_per_location, 4)\n",
    "\n",
    "            #transform generated anchors according to box_transform_pred\n",
    "            proposals= apply_regression_pred_to_anchors_or_proposals(\n",
    "                box_transform_pred.detach().reshape(-1,1,4),\n",
    "                anchors\n",
    "            )\n",
    "\n",
    "            proposals= proposals.reshape(proposals.size(0),4)\n",
    "            propopsals, scores= self.filter_proposals(proposals,\n",
    "                                                    cls_scores.detach(),\n",
    "                                                    image.shape)\n",
    "\n",
    "            rpn_output= {\n",
    "                'proposals': proposals,\n",
    "                'scores': score\n",
    "            }\n",
    "\n",
    "            if not self.training or target is None:\n",
    "                return rpn_output\n",
    "            else:\n",
    "                #in training\n",
    "                #assign gt box and label for each anchor\n",
    "                labels_for_anchors, matched_gt_boxes_for_anchors= self.assign_targets_to_anchors(\n",
    "                    anchors,\n",
    "                    target['bboxes'][0]\n",
    "                )\n",
    "\n",
    "                #based on gt assignment above, get regression targets for anchors\n",
    "                #matched_gt_boxes_for_anchors -> (number of anchors in image, 4)\n",
    "                #anchors -> (number of anchors in image, 4)\n",
    "                regression_targets= boxes_to_transformation_targets(\n",
    "                    matched_gt_boxes_for_anchors,\n",
    "                    anchors\n",
    "                )\n",
    "\n",
    "                #sample positive and negative anchors for training \n",
    "                sampled_neg_idx_mask, sampled_pos_idx_mask= sample_positve_negative(\n",
    "                    labels_for_anchors,\n",
    "                    positive_count= 128,\n",
    "                    total_count= 256\n",
    "                )\n",
    "\n",
    "                sampled_idxs= torch.where(sampled_pos_idx_mask | sampled_neg_idx_mask)[0]\n",
    "                localization_loss= (\n",
    "                    torch.nn.function.smooth_l1_loss(\n",
    "                        box_transform_pred[sampled_pos_idx_mask],\n",
    "                        regression_targets[sampled_pos_idx_mask],\n",
    "                        beta= 1/9,\n",
    "                        reduction= 'sum'\n",
    "                    ) / (sampled_idxs.numel())\n",
    "                )\n",
    "\n",
    "                cls_loss= torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "                    cls_scores[sampled_idxs].flatten90,\n",
    "                    labels_for_anchors[sampled_idxs].flatten()\n",
    "                )\n",
    "\n",
    "                rpn_output['rpn_classification_loss']= cls_loss\n",
    "                rpn_output['rpn_localization_loss']= localization_loss\n",
    "\n",
    "                return rpn_output\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e8ece-3b2c-41d8-814e-c9f528ea9bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROIHead(nn.Module): \n",
    "    def __init__(self, num_classes= 21, in_channels= 512):\n",
    "        super(ROIHead, slef).__init__()\n",
    "        self.num_classes= num_classes\n",
    "        self.pool_size= 7\n",
    "        self.fc_inner_dim= 1024\n",
    "\n",
    "        self.fc6= nn.Linear(in_channels * self.pool_size *self.pool_size, \n",
    "                            self.fc_inner_dim)\n",
    "\n",
    "        self.fc7= nn.Linear(self.fc_inner_dim, self.fc_inner_dim)\n",
    "        self.cls_layer= nn.Linear(self.fc_inner_dim, self.num_classes)\n",
    "        self.bbox_reg_layer= nn.Linear(self.fc_inner_dim, self.num_classes * 4)\n",
    "\n",
    "    \"\"\"\n",
    "    Shapes\n",
    "\n",
    "    feat                 [1x512x37x50]\n",
    "    proposals            [<=2000x4]\n",
    "    image_shape          [600, 800]\n",
    "    target['labels']     [1x6]\n",
    "    target['bboxes']     [1x6x4]\n",
    "    gt_boxes             [6x4]\n",
    "    gt_labels            [6,]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def assign_target_to_proposals(self, proposals, gt_boxes, gt_labels):\n",
    "        iou_matrix= get_iou(gt_boxes, proposals)\n",
    "        best_match_iou, best_match_gt_idx= iou_matrix.max(dim= 0)\n",
    "        below_low_threshhold= best_match_iou < 0.5\n",
    "\n",
    "        best_match_gt_idx[below_low_threshhold]= -1\n",
    "        matched_gt_boxes_for_proposals= gt_boxes[best_match_gt_idx.clamp(min=0)]\n",
    "\n",
    "        labels= gt_labels[best_match_gt_idx.clamp(min=0)]\n",
    "        labels= labels.to(dtype= torch.int32)\n",
    "\n",
    "        background_proposals= best_match_gt_idx == -1\n",
    "    \n",
    "    def forwars(self, feat, proposals, image_shape, target):\n",
    "        if self.training and target is not None:\n",
    "            gt_boxes= target['bboxes'][0]\n",
    "            gt_labels= target['labels'][0]\n",
    "\n",
    "            #assign labels and gt boxes for proposals \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
